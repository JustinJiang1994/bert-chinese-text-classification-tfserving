# Docker Compose 配置文件
# 用于编排BERT中文文本分类微服务架构
# 包含API网关服务和TensorFlow Serving推理服务
version: '3.8'

# 定义服务列表
services:
  # ==================== API网关服务 ====================
  # 负责接收用户请求、文本预处理、调用推理服务、结果后处理
  api-gateway:
    # 构建配置：使用当前目录的Dockerfile构建镜像
    # 这会创建一个包含Flask应用和所有依赖的Python容器
    build: .
    
    # 容器名称：便于识别和管理
    container_name: api-gateway
    
    # 端口映射：将容器内的5001端口映射到宿主机的5001端口
    # 格式：宿主机端口:容器端口
    # 这样外部客户端就可以通过localhost:5001访问API网关
    ports:
      - "5001:5001"
    
    # 卷挂载：将本地文件系统挂载到容器内
    # 格式：宿主机路径:容器内路径
    # 这里挂载saved_model目录，包含分词器和标签映射文件
    # 这样API网关就可以加载BERT分词器进行文本预处理
    volumes:
      - ./saved_model:/app/saved_model
    
    # 环境变量：传递给容器内部的环境变量
    # TF_SERVING_URL：推理服务的完整URL
    # 使用服务名'inference-service'作为主机名，Docker Compose会自动处理DNS解析
    # 这样API网关就可以通过HTTP请求调用推理服务
    environment:
      - TF_SERVING_URL=http://inference-service:8501/v1/models/bert-chinese:predict
    
    # 依赖关系：指定服务启动顺序
    # depends_on确保inference-service在api-gateway之前启动
    # 这样可以避免API网关启动时推理服务还未就绪的问题
    depends_on:
      - inference-service
    
    # 重启策略：容器异常退出时的处理方式
    # unless-stopped：除非手动停止，否则总是重启容器
    # 这确保了服务的持续可用性
    restart: unless-stopped

  # ==================== 推理服务 ====================
  # 负责加载SavedModel并进行高效的模型推理计算
  inference-service:
    # 镜像配置：直接使用TensorFlow官方提供的Serving镜像
    # 这个镜像已经包含了TensorFlow Serving的所有依赖
    # 无需自己构建，直接拉取使用
    image: tensorflow/serving
    
    # 容器名称：便于识别和管理
    container_name: inference-service
    
    # 端口映射：将容器内的8501端口映射到宿主机的8501端口
    # 注意：在生产环境中，推理服务通常不需要对外暴露端口
    # 这里保留是为了调试方便，可以通过localhost:8501直接访问TF Serving
    # 实际生产环境可以注释掉这行，只允许内部网络访问
    ports:
      - "8501:8501"
    
    # 卷挂载：将本地模型文件挂载到容器内
    # TensorFlow Serving期望的模型路径格式：/models/模型名/版本号/
    # 这里将./tf_serving_model挂载到/models，包含bert-chinese/1/目录结构
    volumes:
      - ./tf_serving_model:/models
    
    # 环境变量：配置TensorFlow Serving的行为
    # MODEL_NAME：指定要加载的模型名称
    # TensorFlow Serving会自动在/models目录下查找名为bert-chinese的模型
    environment:
      - MODEL_NAME=bert-chinese
    
    # 重启策略：确保服务持续运行
    restart: unless-stopped

# ==================== 网络配置 ====================
# Docker Compose会自动创建一个默认网络
# 网络名称格式：项目目录名_default
# 所有服务都在这个网络中，可以通过服务名相互访问
# 例如：api-gateway可以通过http://inference-service:8501访问推理服务

# ==================== 数据持久化 ====================
# 通过volumes配置实现数据持久化：
# 1. saved_model目录：包含分词器和标签映射，API网关需要访问
# 2. tf_serving_model目录：包含导出的SavedModel，推理服务需要访问
# 这样即使容器重启，数据也不会丢失

# ==================== 服务发现 ====================
# Docker Compose提供内置的服务发现机制：
# 1. 每个服务都有一个内部DNS名称（服务名）
# 2. 服务间可以通过服务名相互访问
# 3. 无需配置IP地址，Docker自动处理网络路由

# ==================== 扩展性考虑 ====================
# 这个配置支持水平扩展：
# 1. 可以启动多个api-gateway实例（需要负载均衡器）
# 2. 可以启动多个inference-service实例（需要模型同步）
# 3. 可以添加监控、日志收集等服务

# ==================== 生产环境优化建议 ====================
# 1. 移除inference-service的端口映射（安全性）
# 2. 添加资源限制（内存、CPU）
# 3. 添加健康检查
# 4. 配置日志驱动
# 5. 使用外部网络
# 6. 添加环境变量文件(.env) 